---
title: Hadoop で処理するのに向かないデータ
aliases:
  - Hadoop
tags:
  - d/2009/10/21
  - n/PGM/Hadoop
---

- 2009年10月21日


Hadoopで処理するのに向かないデータ
================================================================================
カーディナリティが著しく低いデータの集計
--------------------------------------------------------------------------------
超巨大マスタのプライマリキーが埋まっているであろう超巨大トランザクションデータの集計はHadoopは向かないような「気がする」

ここでのマスタはマスタ件数＝トランザクション件数のような何を管理したいのかよくわからんテーブルのことを言う・・・。っで実際こういうテーブルは結構ある。

Hadoopでは1個のキーでグルーピングされたデータは1個のコンピュータのある特定の1回のReduceフェーズで処理される。

集計軸のカーディナリティが低いと1個のPCの1回のReduceフェーズで処理されるデータ量が数件になってしまう。

Reduceフェーズの起動回数はやたら多いが1回に処理するレコード件数はたいしたことないという状況になり起動コストが上がってしまうということになる。

なのでMapフェーズである程度（JavaVMが使えるメモリ範囲内に収まる）大きさにするのが望ましい。・・・がしかし、各キーでグルーピングされたデータはどのコンピュータでいつのタイミングで処理されるかわからず、キー間での連携はとりにくい。なのでデータの水平分割は難しい。そこで垂直に分割するわけだが、これも手ごろなキーが無いと難しい。

たぶんバージョンがあがればReduceフェーズの自動分割なんかもやってくれるとは思うんだけどね。

キーあたりのレコードが巨大なデータの集計
--------------------------------------------------------------------------------
↑の処理よろしく、1個のVMの上にのっからないぐらいのグルーピングされたデータはきつい
